---
title: "Assignment5"
author: "wooseokYoon"
date: "2023-05-11"
output: 
  html_document: 
    highlight: tango
---

# Handwritten Digit Recognition

MNIST 데이터셋은 image classification model의 성능을 평가하는 데 주로 활용되는 데이터셋으로, 아래 예와 같이 손으로 쓰여진 숫자들의 이미지 70,000개로 구성되어 있다. 이 중에서 60,000개는 training set으로 활용되며 10,000개는 test set으로 활용된다. 각 데이터는 28 \* 28 = 784개의 픽셀의 명암을 0\~255 사이의 값으로 표현한 784개의 feature와 0\~9 사이의 숫자로 표현되는 target을 포함한다. 본 과제에서는 tree를 활용하여 숫자를 분류하기 위한 classification model을 만들어본다.

```{r message = FALSE}
# 사용할 패키지 추가
library(dslabs)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
```

1.  아래의 순서에 따라 data preprocessing을 수행하자.

A. dslabs 패키지를 설치하고, 다음 코드를 실행하면 mnist 변수에 아래 설명과 같이 데이터가 저장된다.

"mnist \<- read_mnist()"

-   mnist$train$images: Training set의 feature 데이터 (행렬)
-   mnist$train$labels: Training set의 Target 데이터 (벡터)
-   mnist$test$images: Test set의 feature 데이터 (행렬)
-   mnist$test$labels: Test set의 Target 데이터 (벡터)

```{r}
# mnist 변수에 손글씨 데이터 할당
mnist <- read_mnist()
```

B. Training set의 데이터 사이즈가 매우 크기 때문에 60,000개의 데이터 중에 처음 2,000개만 사용하자. 이때, feature 데이터는 변수 train_x에 저장하고, target 데이터는 변수 train_y에 저장한다. train_y의 분포를 확인해보자.

```{r}
# 조건에 맞게 training set 만들기
train_x <- mnist$train$images[1:2000,]
train_y <- mnist$train$labels[1:2000]
```

C. train_x의 column의 이름을 V1, V2, V3 ... 순서대로 설정하자. colnames() 함수를 사용하여 column의 이름을 수정할 수 있다.

```{r}
# column의 이름을 조건에 맞게 for문을 이용하여 list에 저장
colname <- list()
for(i in 1 : ncol(train_x)){
  colname <- append(colname, paste0("V", i))
}

# column의 이름 수정
colnames(train_x) <- colname
str(train_x)
```

D. 784개의 픽셀 중에서 숫자와 관련없는 가장자리 부분과 같은 경우는 많은 데이터들에 대해서 같은 색을 가진다. 이러한 픽셀은 숫자를 분류하는 데 크게 영향을 미치지 않으므로 feature에서 제외시키는 것이 합리적이다. caret 패키지의 nearZeroVar(train_x) 함수를 실행하면 train_x의 column들 중에서 variance가 0이거나 0에 가까운 것들의 index를 얻을 수 있다. 이 index에 해당하는 column을 train_x에서 제외시키자. 784개의 feature 중에서 몇 개가 제외되었는가?

```{r}
# 값이 0이거나 0에 가까운 index를 deletecol 변수에 할당
deletecol <- nearZeroVar(train_x)

# deletecol에 해당하는 column을 train_x에서 제외
train_x <- subset(train_x, select = -deletecol)
str(train_x)
```

**Answer) 784개의 feature 중에 560개가 제외되어 224개가 남았다.**

E. 최종적으로 train_x와 train_y를 합쳐서 train이라는 이름의 데이터프레임을 만들자.

```{r}
# train_x와 train_y 열 방향으로 합치기
train <- cbind(train_x, train_y)

# matrix를 dataframe 형식으로 변환
train <- as.data.frame(train)

# target 열을 factor 요인으로 변환 
train$train_y <- as.factor(train$train_y)
```

F. C\~E의 과정을 test set에 대해서 동일하게 수행하여 test라는 이름의 데이터프레임을 만들자. 이때 D에서 제외한 feature와 동일한 feature들을 test set에서도 제외시켜야 한다.

```{r}
# data의 test set을 각각 변수에 할당
test_x <- mnist$test$images
test_y <- mnist$test$labels

# C에서 만든 column 이름 리스트를 이용하여 column의 이름 수정
colnames(test_x) <- colname

# trian set에 deletecol에 해당하는 column을 test_x에서 제외
test_x <- subset(test_x, select = -deletecol)

# train_x와 train_y 열 방향으로 합치기
test <- cbind(test_x, test_y)

# matrix를 dataframe 형식으로 변환
test <- as.data.frame(test)

# target 열을 factor 요인으로 변환 
test$test_y <- as.factor(test$test_y)
```

2.  아래의 코드는 test set의 첫번째 데이터를 화면에 이미지로 출력해준다. 이를 활용하여 test set의 image 행렬의 행 번호를 입력받아 숫자 이미지를 출력하는 함수 print_image()를 만들어보자. 이 함수를 활용하여 test set 중에서 이미지로부터 실제 숫자값을 유추하기 어려운 예를 몇 개 찾아보자.

```{r}
#실제값이 3인데 8처럼 보이는 경우
image(1:28, 1:28, matrix(mnist$test$images[1115,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")

#실제값이 8인데 9처럼 보이는 경우
image(1:28, 1:28, matrix(mnist$test$images[3290,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")

#실제값이 3인데 5처럼 보이는 경우
image(1:28, 1:28, matrix(mnist$test$images[4741,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")

# 실제 값이 4인데 9처럼 보이는 경우
image(1:28, 1:28, matrix(mnist$test$images[1454,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")
```

3.  아래의 순서로 tree를 만들어보자.

A. Cost complexity parameter 일때, leaf node가 가지는 최소 데이터의 수가 80인 Tree를 만들고 시각화해보자. Tree는 몇 개의 leaf node를 가지는가? Tree의 depth는 얼마인가?

```{r}
set.seed(77)
# leaf node가 가지는 최소 데이터수 80인 tree 생성
mnist_ct1 <- rpart(train_y~., data = train, method="class", minbucket = 80, control=list(cp=0))

# tree 시각화
rpart.plot(mnist_ct1, box.palette = 0)
```

**Answer) 10개의 node를 가지고 depth는 4다.**

B. Cost complexity parameter 일때, depth가 최대 3인 Tree를 만들고 시각화해보자. Tree는 몇개의 leaf node를 가지는가? 만들어진 tree가 실제 classification에 활용될 수 있을까?

```{r}
set.seed(77)
# 최대 depth가 3인 tree 생성
mnist_ct2 <- rpart(train_y~., data = train, method="class", maxdepth = 3, control=list(cp=0))

# tree 시각화
rpart.plot(mnist_ct2, box.palette = 0)
```

**Answer) 활용될 수 없을 것이다.**

**첫째로 분류되지 않은 클래스가 존재한다. 10개의 클래스를 구분해야하는데, 8개의 leaf node를 가지고 있어 분류되지 않는다. 현재 tree에서는 2,5,9 클래스로 분류되는 node는 존재하지 않다.**

**둘째로 leaf node의 비율을 보더라도 상대적으로 높아서 특정 숫자 클래스로 분류해준 것이지 실제로 분류되기에는 너무 낮은 비율을 가지는 node들이 많다. 좌측에서부터 2번째 leaf node는 4로 분류되기는 했지만 실제 4는 19%만을 포함하고 있다. 다음으로 6번째 leaf node는 8로 분류되었지만 실제 8은 26%만을 포함하고 있다.**

C. rpart() 함수의 default 옵션으로 Tree를 만든 후 cross validation을 활용한 pruning 과정을 수행해보자.

```{r}
set.seed(77)
# default 옵션으로 tree 생성
mnist_ct3 <- rpart(train_y~., data = train, method="class")

# tree 시각화
rpart.plot(mnist_ct3, box.palette = 0)

# cross validation의 결과
printcp(mnist_ct3)

# cp에 따른 cross validation의 xerror 결과 시각화
plotcp(mnist_ct3)

# xerror가 가장 낮은 cp값 변수로 할당
best_cp <- mnist_ct3$cptable[which.min(mnist_ct3$cptable[,"xerror"]),"CP"]
best_cp

# best cp일 때 pruned tree 생성
best_ct <- prune(mnist_ct3, cp = best_cp)
best_ct

# pruned tree 시각화
rpart.plot(best_ct, box.palette = 0)
```

D. C에서 얻은 tree로 test set에 대한 예측을 수행하고, confusion matrix를 계산해보자. Test set에 대한 예측정확도는 얼마인가?

```{r}
# best cp일 때 pruned tree의 test set에 대한 예측
pred_class <- predict(best_ct, newdata = test, type = "class")

# 이 때, confusion Matrix
confusionMatrix(pred_class, test$test_y)
```

**Answer) Accuracy는 0.5906이다.**

4.  Random Forest를 만들어보자.

A. randomForest() 함수를 사용하여 bagging model을 만들어보자. mtry를 제외한 옵션은 모두 default 값을 사용한다. plot() 함수를 사용하여 Bagging model에서 tree의 수의 증가에 따른 OOB classification error rate의 변화를 그래프로 출력해보자. 어떤 경향을 보이는가?

```{r}
set.seed(77)
# bagging model 조건으로 tree 생성
bag <- randomForest(train_y~., data = train, mtry = 244)
bag

# tree의 개수에 따른 OOB MSE 시각화
plot(bag)
```

**Answer) tree의 수가 많아질 수록 OOB classification error rate가 줆을 알 수 있다. 그리고 특정 tree의 수를 기점으로 error 감소량이 적어지는 경향이 보인다.**

B. Bagging model로 test set에 대한 예측을 수행하고, confusion matrix를 계산해보자. Test set에 대한 예측 정확도는 얼마인가? 3번에서 계산한 tree model에 비해서 성능이 얼마나 향상되었는가?

```{r}
# bagging model의 test set에 대한 예측
pred_class2 <- predict(bag, newdata = test, type = "class")

# 이 때, confusion Matrix
confusionMatrix(pred_class2, test$test_y)
```

**Answer) Accuracy가 0.5906에서 0.8944로 증가하였다. Bagging을 사용하면 tree를 여러 개 생성하여 variance를 줄이는 역할을 하여 예측 정확도를 올릴 수 있다는 장점이 있다.**

C. randomForest() 함수의 default 옵션을 사용하여 random forest model을 만들어보자. 그리고 Bagging과 random forest 모델의 Tree의 수의 증가에 따른 OOB classification error rate의 변화를 하나의 그래프에 그려보고 두 모델의 성능을 비교해보자.

```{r}
set.seed(77) 
# mtry값이 default 값인 random forest model 생성
rf <- randomForest(train_y~., data = train) 
rf

# Bagging과 random forest model의 Tree의 수의 변화에 따른 OOB classification error rate 시각화
# bagging model의 class : darkred, random forest model의 class : blue
plot(bag, col = "darkred")
plot(rf, col = "blue", add = TRUE)
```

**Answer) 그래프를 보면 경향은 비슷하지만 파랑색 선(random forest model)이 암적색 선(bagging model)보다 더 낮은 error 위치에서 그려지고 있음을 확인할 수 있다. 즉, OOB classification error rate가 random forest model이 더 낮음을 알 수 있고 이에 따라 성능도 더 좋다는 것을 확인할 수 있다.**

D. Random forest model로 test set에 대한 예측을 수행하고, confusion matrix를 계산해보자. Test set에 대한 예측 정확도는 얼마인가? Bagging model에 비해서 성능이 얼마나 향상되었는가?

```{r}
#  mtry값이 default 값인 random forest model의 test set에 대한 예측
pred_class <- predict(rf, newdata = test, type = "class")

# 이 때, confusion Matrix
confusionMatrix(factor(pred_class), factor(test$test_y))
```

**Answer) Accuracy가 0.8944에서 0.911로 증가하였다. Bagging model을 사용하게 되면 target에 영향이 큰 feature들을 위주로 tree가 형성되기 때문에 다수의 tree가 유사한 형태를 가지게 될 것이다. 이를 feature의 수를 정해놓고 feature를 random하게 선택하게 되면 각 tree들의 상관관계가 줄어 variance를 더 줄이는 효과가 나타나 더 높은 예측 정확도를 기대할 수 있다.**

E. D번의 confusion matrix 결과로부터, 분류가 가장 정확한 숫자는 몇인가? 가장 분류가 어려운 숫자는 몇인가?

**Answer) 분류가 가장 정확한 숫자는 1이다. 1은 Senitivity와 Specificity가 모두 높은 것으로 보아 1을 다른 수로 분류하는 빈도와 다른 수를 1로 분류하는 빈도가 모두 적다.**

**분류가 가장 어려운 숫자는 8이다. 8로 예측했을 때, 실제로 8인 경우로 예측되는 Specificity는 높은 편이지만, 실제로 8인 경우에 다른 수로 예측되는 경우가 많다. 오판인 경우 9로 예측되는 경우가 가장 많으며, 이외에도 다른 수로도 오판되는 경우가 많다.**

F. 실제 값은 9지만 Random forest model에 의해 0으로 예측되는 test data를 찾아 이미지를 몇 개 출력해보자. 눈으로 확인했을 때 9와 0의 구별이 어려운가?

```{r}
# 실제 값은 9지만 예측은 0으로 예측한 행의 index
rownames(subset(test, test$test_y == 9 & pred_class == 0))

# 예시 1
image(1:28, 1:28, matrix(mnist$test$images[1248,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")

# 예시 2
image(1:28, 1:28, matrix(mnist$test$images[2381,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")

# 예시 3
image(1:28, 1:28, matrix(mnist$test$images[2649,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")

# 예시 4
image(1:28, 1:28, matrix(mnist$test$images[4164,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")

# 예시 5
image(1:28, 1:28, matrix(mnist$test$images[4875,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")

# 예시 6
image(1:28, 1:28, matrix(mnist$test$images[6506,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")

# 예시 7
image(1:28, 1:28, matrix(mnist$test$images[6741,], nrow=28)[ , 28:1], col = gray(seq(0, 1, 0.05)), xlab = "", ylab="")
```

**Answer) 예시 2, 6, 7은 원래의 9보다 밑에 꼭지 부분이 짧아 애매하기는하지만 대체로 꼭지 부분이 보이고 원 부분도 식별이 되기 때문에 눈으로 구별이 어렵지는 않다.**
